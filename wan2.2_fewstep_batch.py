from pipeline import Wan22FewstepInferencePipeline
from diffusers.utils import export_to_video
from omegaconf import OmegaConf
import argparse
import torch
import os
import csv
import torchvision.transforms.functional as TF
from PIL import Image
import math

def process_one(prompt, image, seed, idx):
    os.makedirs("outputs", exist_ok=True)
    out_path = f"outputs/line_{idx:03d}_video.mp4"

    target_w, target_h = args.w, args.h
    wan22_image_latent = None

    if image is not None and os.path.exists(image):
        img = Image.open(image).convert("RGB")
        orig_w, orig_h = img.size
        max_pixels = 704 * 1280
        ori_pixels = orig_w * orig_h
        if ori_pixels <= max_pixels:
            target_w, target_h = orig_w, orig_h
        else:
            aspect_ratio = orig_w / orig_h
            target_h = int(math.sqrt(max_pixels / aspect_ratio))
            target_w = int(target_h * aspect_ratio)

        target_w = (target_w // 32) * 32
        target_h = (target_h // 32) * 32

        print(f"ðŸ–¼ï¸ è¾“å…¥å›¾åƒå°ºå¯¸: ({orig_w}, {orig_h})ï¼Œå°†è‡ªåŠ¨è°ƒæ•´ä¸º: ({target_w}, {target_h}) ä»¥åŒ¹é…é•¿å®½æ¯”å¹¶æ»¡è¶³æœ€å¤§åƒç´ é™åˆ¶ã€‚")

        img_resized = img.resize((target_w, target_h), Image.LANCZOS)
        img_tensor = TF.to_tensor(img_resized).sub_(0.5).div_(0.5).to("cuda").unsqueeze(1).to(dtype=torch.bfloat16)
        wan22_image_latent = pipe.vae.encode_to_latent(img_tensor.unsqueeze(0))
    else:
        if image is not None:
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°å›¾åƒè·¯å¾„ '{image}'ï¼Œå°†ä½¿ç”¨é»˜è®¤å°ºå¯¸è¿›è¡ŒT2Vç”Ÿæˆã€‚")
        print(f"â„¹ï¸ æœªæä¾›è¾“å…¥å›¾åƒæˆ–è·¯å¾„æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å°ºå¯¸: ({target_w}, {target_h})")


    # ä½¿ç”¨ç›®æ ‡å°ºå¯¸ç”Ÿæˆè§†é¢‘
    video = (
        pipe.inference(
            noise=torch.randn(
                1,
                (args.num_frames - 1) // 4 + 1,
                48,
                target_h // 16, # ä½¿ç”¨è®¡ç®—å‡ºçš„æˆ–é»˜è®¤çš„é«˜åº¦
                target_w // 16, # ä½¿ç”¨è®¡ç®—å‡ºçš„æˆ–é»˜è®¤çš„å®½åº¦
                generator=torch.Generator(device="cuda").manual_seed(seed),
                dtype=torch.bfloat16,
                device="cuda",
            ),
            text_prompts=[prompt],
            wan22_image_latent=wan22_image_latent,
        )[0]
        .permute(0, 2, 3, 1)
        .cpu()
        .numpy()
    )
    export_to_video(video, out_path, fps=24)
    print(f"âœ… å·²ç”Ÿæˆï¼š{out_path}")

# --- ä¸»ç¨‹åºéƒ¨åˆ†ä¿æŒä¸å˜ ---

parser = argparse.ArgumentParser()
parser.add_argument("--config_path", type=str)
parser.add_argument("--checkpoint_folder", type=str, default=None)
parser.add_argument("--seed", type=int, default=43)
# æ›´æ–°äº†å¸®åŠ©ä¿¡æ¯ä»¥è¯´æ˜Žå…¶ä½œä¸ºé»˜è®¤å€¼çš„ä½œç”¨
parser.add_argument("--h", type=int, default=704, help="è§†é¢‘çš„é»˜è®¤é«˜åº¦ã€‚åœ¨I2Væ¨¡å¼ä¸‹ï¼Œä¼šæ ¹æ®è¾“å…¥å›¾ç‰‡è‡ªåŠ¨è®¡ç®—ã€‚")
parser.add_argument("--w", type=int, default=1280, help="è§†é¢‘çš„é»˜è®¤å®½åº¦ã€‚åœ¨I2Væ¨¡å¼ä¸‹ï¼Œä¼šæ ¹æ®è¾“å…¥å›¾ç‰‡è‡ªåŠ¨è®¡ç®—ã€‚")
parser.add_argument("--num_frames", type=int, default=121)
parser.add_argument("--csv", type=str, default=None, help="ç”¨äºŽæ‰¹é‡æŽ¨ç†çš„CSVæ–‡ä»¶è·¯å¾„")
args = parser.parse_args()
assert args.num_frames % 4 == 1, "num_frameså¿…é¡»æ˜¯4çš„å€æ•°åŠ 1"


torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.set_grad_enabled(False)

config = OmegaConf.load(args.config_path)

pipe = Wan22FewstepInferencePipeline(config)
if args.checkpoint_folder is not None:
    model_path = os.path.join(args.checkpoint_folder, "model.pt")
    model_path2 = os.path.join(args.checkpoint_folder, "Wan2.2-TI2V-5B.pth")
    if os.path.exists(model_path):
        print(f"Loading checkpoint from: {model_path}, format .pt")
        state_dict = torch.load(model_path, map_location="cpu")
    elif os.path.exists(model_path2):
        print(f"Loading checkpoint from: {model_path2}, format .pth")
        state_dict = torch.load(model_path2, map_location="cpu")
    state_dict = state_dict["generator_ema"] if "generator_ema" in state_dict else state_dict
    
    new_state_dict = {}
    for key, value in state_dict.items():
        if not key.startswith("model"):
            key = "model." + key
        new_key = key.replace("_fsdp_wrapped_module.", "")
        new_key = new_key.replace("_checkpoint_wrapped_module.", "")
        new_key = new_key.replace("_orig_mod.", "")
        new_state_dict[new_key] = value
    m, u = pipe.generator.load_state_dict(new_state_dict, strict=False)
    assert len(u) == 0, f"Unexpected keys in state_dict: {u}"
pipe = pipe.to(device="cuda", dtype=torch.bfloat16)

if args.csv is not None:
    with open(args.csv, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for idx, row in enumerate(reader):
            prompt = row["prompt"]
            # ç¡®ä¿imageå­—æ®µä¸ºç©ºæ—¶ä¼ é€’None
            image = row["image"] if row.get("image") else None
            seed = int(row["seed"])
            print(f"ðŸš€ æ­£åœ¨å¤„ç†ç¬¬ {idx+1} è¡Œï¼š{prompt[:50]}...")
            process_one(prompt, image, seed, idx+1)